{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817666f3-90be-428a-85ff-13b861920557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "class IncideCoderScraper:\n",
    "    def __init__(self, base_url: str = \"https://incidecoder.com\", delay_range: tuple = (1, 3)):\n",
    "        \"\"\"\n",
    "        Initialize the IncideCoder scraper.\n",
    "        \n",
    "        Args:\n",
    "            base_url: The base URL of the IncideCoder website\n",
    "            delay_range: Tuple of (min, max) seconds to wait between requests\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.delay_range = delay_range\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        self.products = []\n",
    "        \n",
    "    def get_soup(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"\n",
    "        Make a request to the URL and return a BeautifulSoup object.\n",
    "        \n",
    "        Args:\n",
    "            url: The URL to request\n",
    "            \n",
    "        Returns:\n",
    "            BeautifulSoup object or None if request failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error retrieving {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_product_links(self, category_url: str, max_pages: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Scrape product links from category pages.\n",
    "        \n",
    "        Args:\n",
    "            category_url: URL of the category to scrape\n",
    "            max_pages: Maximum number of pages to scrape\n",
    "            \n",
    "        Returns:\n",
    "            List of product URLs\n",
    "        \"\"\"\n",
    "        product_links = []\n",
    "        page = 1\n",
    "        \n",
    "        while page <= max_pages:\n",
    "            url = f\"{category_url}/{page}\" if page > 1 else category_url\n",
    "            print(f\"Scraping page {page}: {url}\")\n",
    "            \n",
    "            soup = self.get_soup(url)\n",
    "            if not soup:\n",
    "                break\n",
    "                \n",
    "            # Find product links on the page\n",
    "            product_elements = soup.select('.detailPageLinkOnly')\n",
    "            if not product_elements:\n",
    "                print(f\"No product elements found on page {page}\")\n",
    "                break\n",
    "                \n",
    "            for element in product_elements:\n",
    "                link = element.get('href')\n",
    "                if link:\n",
    "                    product_links.append(urljoin(self.base_url, link))\n",
    "            \n",
    "            print(f\"Found {len(product_elements)} products on page {page}\")\n",
    "            \n",
    "            # Check if there's a next page\n",
    "            next_page = soup.select_one('.pagination .next')\n",
    "            if not next_page or 'disabled' in next_page.get('class', []):\n",
    "                print(\"No more pages available\")\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            # Add delay to avoid overloading the server\n",
    "            time.sleep(random.uniform(*self.delay_range))\n",
    "            \n",
    "        return product_links\n",
    "    \n",
    "    def extract_ingredients(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract ingredients from the product page.\n",
    "        \n",
    "        Args:\n",
    "            soup: BeautifulSoup object of the product page\n",
    "            \n",
    "        Returns:\n",
    "            List of ingredient dictionaries with name and function\n",
    "        \"\"\"\n",
    "        ingredients_section = soup.select_one('.detailpage-content-ingredientlist')\n",
    "        if not ingredients_section:\n",
    "            return []\n",
    "        \n",
    "        ingredients = []\n",
    "        ingredient_items = ingredients_section.select('.ingred-cell')\n",
    "        \n",
    "        for item in ingredient_items:\n",
    "            ingredient_data = {}\n",
    "            \n",
    "            # Extract ingredient name\n",
    "            name_elem = item.select_one('.ingred-link')\n",
    "            if name_elem:\n",
    "                ingredient_data['name'] = name_elem.text.strip()\n",
    "            else:\n",
    "                continue  # Skip if no name found\n",
    "                \n",
    "            # Extract ingredient function if available\n",
    "            func_elem = item.select_one('.itemprop-func')\n",
    "            if func_elem:\n",
    "                ingredient_data['function'] = func_elem.text.strip()\n",
    "            else:\n",
    "                ingredient_data['function'] = ''\n",
    "                \n",
    "            ingredients.append(ingredient_data)\n",
    "        \n",
    "        return ingredients\n",
    "    \n",
    "    def extract_product_attributes(self, soup: BeautifulSoup) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract product attributes like safety, suitability for skin types, and functions.\n",
    "        \n",
    "        Args:\n",
    "            soup: BeautifulSoup object of the product page\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of product attributes\n",
    "        \"\"\"\n",
    "        attributes = {\n",
    "            'safety': 'N/A',\n",
    "            'oily': False,\n",
    "            'dry': False,\n",
    "            'sensitive': False,\n",
    "            'comedogenic': False,\n",
    "            'acne_fighting': False,\n",
    "            'anti_aging': False,\n",
    "            'brightening': False,\n",
    "            'uv': False\n",
    "        }\n",
    "        \n",
    "        # Safety rating\n",
    "        safety_element = soup.select_one('.product-safety-rating')\n",
    "        if safety_element:\n",
    "            safety_text = safety_element.text.strip()\n",
    "            safety_match = re.search(r'(\\d+)/\\d+', safety_text)\n",
    "            if safety_match:\n",
    "                attributes['safety'] = safety_match.group(1)\n",
    "        \n",
    "        # Product details and description\n",
    "        product_details = soup.select('.product-details-text')\n",
    "        details_text = ' '.join([detail.text.lower().strip() for detail in product_details])\n",
    "        \n",
    "        product_description = soup.select_one('.product-description')\n",
    "        if product_description:\n",
    "            description_text = product_description.text.lower().strip()\n",
    "            details_text += ' ' + description_text\n",
    "        \n",
    "        # Skin types\n",
    "        if any(term in details_text for term in ['oily skin', 'for oily', 'oily complexion']):\n",
    "            attributes['oily'] = True\n",
    "        if any(term in details_text for term in ['dry skin', 'for dry', 'dry complexion']):\n",
    "            attributes['dry'] = True\n",
    "        if any(term in details_text for term in ['sensitive skin', 'for sensitive']):\n",
    "            attributes['sensitive'] = True\n",
    "            \n",
    "        # Functions\n",
    "        if any(term in details_text for term in ['comedogenic', 'non-comedogenic', 'noncomedogenic']):\n",
    "            attributes['comedogenic'] = True\n",
    "        if any(term in details_text for term in ['acne', 'pimple', 'breakout', 'blemish']):\n",
    "            attributes['acne_fighting'] = True\n",
    "        if any(term in details_text for term in ['anti-aging', 'anti aging', 'wrinkle', 'fine lines', 'aging']):\n",
    "            attributes['anti_aging'] = True\n",
    "        if any(term in details_text for term in ['brighten', 'brightening', 'illuminating', 'radiance', 'glow']):\n",
    "            attributes['brightening'] = True\n",
    "        if any(term in details_text for term in ['spf', 'sunscreen', 'uv protection', 'sun protection']):\n",
    "            attributes['uv'] = True\n",
    "            \n",
    "        return attributes\n",
    "    \n",
    "    def scrape_product_details(self, product_url: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Scrape details for a single product.\n",
    "        \n",
    "        Args:\n",
    "            product_url: URL of the product to scrape\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of product details or None if scraping failed\n",
    "        \"\"\"\n",
    "        print(f\"Scraping product: {product_url}\")\n",
    "        soup = self.get_soup(product_url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        # Extract basic product information\n",
    "        product_info = {\n",
    "            'id': product_url.split('/')[-1],\n",
    "            'url': product_url,\n",
    "            'name': '',\n",
    "            'brand': '',\n",
    "            'type': '',\n",
    "            'image': '',\n",
    "            'ingredients': [],\n",
    "            'raw_ingredients_text': ''\n",
    "        }\n",
    "        \n",
    "        # Product name\n",
    "        name_element = soup.select_one('h1.ingredientHeading')\n",
    "        if name_element:\n",
    "            product_info['name'] = name_element.text.strip()\n",
    "            \n",
    "        # Brand\n",
    "        brand_element = soup.select_one('.detailpage-brand a')\n",
    "        if brand_element:\n",
    "            product_info['brand'] = brand_element.text.strip()\n",
    "            \n",
    "        # Product type\n",
    "        type_element = soup.select_one('.product-details-category')\n",
    "        if type_element:\n",
    "            product_info['type'] = type_element.text.strip()\n",
    "            \n",
    "        # Product image\n",
    "        image_element = soup.select_one('.detailpage-img img')\n",
    "        if image_element and image_element.has_attr('src'):\n",
    "            product_info['image'] = urljoin(self.base_url, image_element['src'])\n",
    "            \n",
    "        # Raw ingredients text\n",
    "        ingredients_text_element = soup.select_one('.show-more-text')\n",
    "        if ingredients_text_element:\n",
    "            product_info['raw_ingredients_text'] = ingredients_text_element.text.strip()\n",
    "            \n",
    "        # Ingredients\n",
    "        product_info['ingredients'] = self.extract_ingredients(soup)\n",
    "        \n",
    "        # Other attributes\n",
    "        attributes = self.extract_product_attributes(soup)\n",
    "        product_info.update(attributes)\n",
    "        \n",
    "        # Add delay to avoid overloading the server\n",
    "        time.sleep(random.uniform(*self.delay_range))\n",
    "        \n",
    "        return product_info\n",
    "    \n",
    "    def scrape_category(self, category_url: str, max_pages: int = 5, max_products: int = 100) -> None:\n",
    "        \"\"\"\n",
    "        Scrape products from a category.\n",
    "        \n",
    "        Args:\n",
    "            category_url: URL of the category to scrape\n",
    "            max_pages: Maximum number of pages to scrape\n",
    "            max_products: Maximum number of products to scrape\n",
    "        \"\"\"\n",
    "        product_links = self.scrape_product_links(category_url, max_pages)\n",
    "        print(f\"Found {len(product_links)} product links\")\n",
    "        \n",
    "        count = 0\n",
    "        for link in product_links:\n",
    "            if count >= max_products:\n",
    "                break\n",
    "                \n",
    "            product_info = self.scrape_product_details(link)\n",
    "            if product_info:\n",
    "                self.products.append(product_info)\n",
    "                count += 1\n",
    "                \n",
    "                # Save progress every 10 products\n",
    "                if count % 10 == 0:\n",
    "                    print(f\"Scraped {count} products so far\")\n",
    "                    self.save_to_json(f\"incidecoder_products_progress_{len(self.products)}.json\")\n",
    "    \n",
    "    def save_to_csv(self, filename: str = 'incidecoder_products.csv') -> None:\n",
    "        \"\"\"\n",
    "        Save scraped products to CSV file.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name of the CSV file to save\n",
    "        \"\"\"\n",
    "        if not self.products:\n",
    "            print(\"No products to save\")\n",
    "            return\n",
    "            \n",
    "        fieldnames = ['id', 'name', 'brand', 'type', 'image', 'safety', \n",
    "                      'oily', 'dry', 'sensitive', 'comedogenic', \n",
    "                      'acne_fighting', 'anti_aging', 'brightening', 'uv', \n",
    "                      'url', 'raw_ingredients_text']\n",
    "        \n",
    "        # Add ingredients as separate columns\n",
    "        products_with_flat_ingredients = []\n",
    "        \n",
    "        for product in self.products:\n",
    "            product_copy = product.copy()\n",
    "            \n",
    "            # Extract ingredients and flatten\n",
    "            ingredients_list = product_copy.pop('ingredients', [])\n",
    "            ingredient_names = [ing['name'] for ing in ingredients_list if 'name' in ing]\n",
    "            product_copy['ingredients'] = ', '.join(ingredient_names)\n",
    "            \n",
    "            products_with_flat_ingredients.append(product_copy)\n",
    "        \n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for product in products_with_flat_ingredients:\n",
    "                writer.writerow(product)\n",
    "                \n",
    "        print(f\"Saved {len(self.products)} products to {filename}\")\n",
    "    \n",
    "    def save_to_json(self, filename: str = 'incidecoder_products.json') -> None:\n",
    "        \"\"\"\n",
    "        Save scraped products to JSON file.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name of the JSON file to save\n",
    "        \"\"\"\n",
    "        if not self.products:\n",
    "            print(\"No products to save\")\n",
    "            return\n",
    "            \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.products, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "        print(f\"Saved {len(self.products)} products to {filename}\")\n",
    "    \n",
    "    def load_from_json(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Load products from a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name of the JSON file to load\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"File {filename} does not exist\")\n",
    "            return\n",
    "            \n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            self.products = json.load(f)\n",
    "            \n",
    "        print(f\"Loaded {len(self.products)} products from {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create scraper instance\n",
    "    scraper = IncideCoderScraper()\n",
    "    \n",
    "    # Define categories to scrape\n",
    "    categories = [\n",
    "        'https://incidecoder.com/products/product-type/moisturizer',\n",
    "        'https://incidecoder.com/products/product-type/serum',\n",
    "        'https://incidecoder.com/products/product-type/cleanser',\n",
    "        'https://incidecoder.com/products/product-type/mask'\n",
    "    ]\n",
    "    \n",
    "    # Scrape each category\n",
    "    for category in categories:\n",
    "        # Extract category name from URL for logging\n",
    "        category_name = category.split('/')[-1]\n",
    "        print(f\"\\nScraping category: {category_name}\")\n",
    "        \n",
    "        # Scrape the category\n",
    "        scraper.scrape_category(category, max_pages=2, max_products=25)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        scraper.save_to_json(f\"incidecoder_{category_name}.json\")\n",
    "    \n",
    "    # Save final results in both formats\n",
    "    scraper.save_to_csv(\"incidecoder_products.csv\")\n",
    "    scraper.save_to_json(\"incidecoder_products.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
